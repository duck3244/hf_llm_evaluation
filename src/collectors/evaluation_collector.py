"""
LLM í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ê¸° ëª¨ë“ˆ
README ì˜ˆì‹œì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •ë¨
"""

import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Any, Tuple
import pandas as pd

from ..api.huggingface_api import HuggingFaceAPI, get_api_client
from ..database.db_manager import DatabaseManager
from ..models.data_models import (
    ModelInfo, EvaluationResult, TaskCategory, CollectionStats
)
from ..utils.logger import get_logger, ProgressLogger
from config import Config

logger = get_logger(__name__)


class CollectionError(Exception):
    """ë°ì´í„° ìˆ˜ì§‘ ê´€ë ¨ ì˜ˆì™¸"""
    pass


class LLMEvaluationCollector:
    """LLM í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ê¸° ë©”ì¸ í´ë˜ìŠ¤ (README ì˜ˆì‹œ êµ¬í˜„)"""

    def __init__(self, hf_token: Optional[str] = None, db_path: Optional[str] = None):
        """
        ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”

        Args:
            hf_token: HuggingFace API í† í° (ì„ íƒì‚¬í•­)
            db_path: ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        self.hf_api = get_api_client(hf_token or Config.HUGGINGFACE_TOKEN)
        self.db_manager = DatabaseManager(db_path or Config.DATABASE_PATH)
        self.config = Config()

        # íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™”
        self._initialize_task_categories()

        # ìˆ˜ì§‘ í†µê³„
        self.collection_stats = CollectionStats()

        logger.info("âœ… LLM í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_task_categories(self):
        """íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì´ˆê¸°í™”"""
        logger.debug("íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™” ì¤‘...")

        for task_name, task_info in self.config.TASK_CATEGORIES.items():
            task_category = TaskCategory(
                task_name=task_name,
                description=task_info["description"],
                common_datasets=task_info["common_datasets"],
                common_metrics=task_info["common_metrics"]
            )
            self.db_manager.insert_task_category(task_category)

        logger.debug("íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")

    def collect_models_by_task(self, task: str, limit: int = None) -> List[ModelInfo]:
        """
        íŠ¹ì • íƒœìŠ¤í¬ì˜ ëª¨ë¸ë“¤ì„ ìˆ˜ì§‘ (README ì˜ˆì‹œ êµ¬í˜„)

        Args:
            task: íƒœìŠ¤í¬ ì´ë¦„ (ì˜ˆ: "text-generation")
            limit: ìˆ˜ì§‘í•  ëª¨ë¸ ìˆ˜ ì œí•œ

        Returns:
            ìˆ˜ì§‘ëœ ModelInfo ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        if limit is None:
            limit = self.config.MODELS_PER_TASK

        if task not in self.config.TASKS_TO_COLLECT:
            raise CollectionError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒœìŠ¤í¬: {task}")

        logger.info(f"ğŸ“¥ íƒœìŠ¤í¬ '{task}' ëª¨ë¸ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {limit}ê°œ)")
        start_time = time.time()

        try:
            # HuggingFace APIì—ì„œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            models_data = self.hf_api.get_models(task=task, limit=limit)

            if not models_data:
                logger.warning(f"íƒœìŠ¤í¬ '{task}'ì—ì„œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []

            models = []
            errors = 0

            # ì§„í–‰ë¥  í‘œì‹œ
            progress = ProgressLogger(len(models_data), logger, f"íƒœìŠ¤í¬ {task} ìˆ˜ì§‘")

            for i, model_data in enumerate(models_data, 1):
                try:
                    model_id = model_data.get('id', 'unknown')
                    logger.debug(f"ëª¨ë¸ ì²˜ë¦¬ ì¤‘: {model_id}")

                    # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    detailed_info = self.hf_api.get_model_info(model_id)
                    if not detailed_info:
                        logger.warning(f"ëª¨ë¸ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ: {model_id}")
                        errors += 1
                        progress.update()
                        continue

                    # ModelInfo ê°ì²´ ìƒì„±
                    model_info = self._create_model_info(detailed_info, task)

                    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    if self.db_manager.insert_model(model_info):
                        models.append(model_info)
                        logger.debug(f"âœ“ ëª¨ë¸ ìˆ˜ì§‘ ì™„ë£Œ: {model_info.model_id}")
                    else:
                        logger.warning(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {model_id}")
                        errors += 1

                    # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                    time.sleep(self.config.API_DELAY)
                    progress.update()

                except Exception as e:
                    logger.error(f"ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({model_data.get('id', 'unknown')}): {e}")
                    errors += 1
                    progress.update()
                    continue

            # ìˆ˜ì§‘ ì™„ë£Œ
            progress.finish()
            duration = time.time() - start_time

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.collection_stats.total_models += len(models)
            self.collection_stats.errors_count += errors
            if task not in self.collection_stats.tasks_collected:
                self.collection_stats.tasks_collected.append(task)

            logger.info(f"âœ… íƒœìŠ¤í¬ '{task}' ìˆ˜ì§‘ ì™„ë£Œ:")
            logger.info(f"   â€¢ ì„±ê³µ: {len(models)}ê°œ ëª¨ë¸")
            logger.info(f"   â€¢ ì˜¤ë¥˜: {errors}ê°œ")
            logger.info(f"   â€¢ ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ")

            return models

        except Exception as e:
            logger.error(f"íƒœìŠ¤í¬ '{task}' ìˆ˜ì§‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            raise CollectionError(f"íƒœìŠ¤í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def collect_evaluations_for_model(self, model_id: str, max_evaluations: int = None) -> List[EvaluationResult]:
        """
        íŠ¹ì • ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ìˆ˜ì§‘

        Args:
            model_id: ëª¨ë¸ ID
            max_evaluations: ìµœëŒ€ í‰ê°€ ê²°ê³¼ ìˆ˜

        Returns:
            ìˆ˜ì§‘ëœ EvaluationResult ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        if max_evaluations is None:
            max_evaluations = self.config.MAX_EVALUATIONS_PER_MODEL

        logger.debug(f"ğŸ“Š ëª¨ë¸ '{model_id}' í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")

        try:
            evaluations = self.hf_api.get_model_evaluations(model_id)
            saved_evaluations = []

            for evaluation in evaluations[:max_evaluations]:
                if self.db_manager.insert_evaluation(evaluation):
                    saved_evaluations.append(evaluation)
                    logger.debug(f"âœ“ í‰ê°€ ê²°ê³¼ ì €ì¥: {evaluation.metric_name}={evaluation.metric_value}")

            self.collection_stats.total_evaluations += len(saved_evaluations)

            if saved_evaluations:
                logger.debug(f"âœ… ëª¨ë¸ '{model_id}' í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ: {len(saved_evaluations)}ê°œ")

            return saved_evaluations

        except Exception as e:
            logger.error(f"ëª¨ë¸ '{model_id}' í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def collect_all_tasks(self, tasks: Optional[List[str]] = None,
                          models_per_task: Optional[int] = None) -> CollectionStats:
        """
        ëª¨ë“  íƒœìŠ¤í¬ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ (README ì˜ˆì‹œ êµ¬í˜„)

        Args:
            tasks: ìˆ˜ì§‘í•  íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸ (Noneì‹œ ê¸°ë³¸ íƒœìŠ¤í¬)
            models_per_task: íƒœìŠ¤í¬ë‹¹ ìˆ˜ì§‘í•  ëª¨ë¸ ìˆ˜

        Returns:
            ìˆ˜ì§‘ í†µê³„ ì •ë³´
        """
        if tasks is None:
            tasks = self.config.TASKS_TO_COLLECT
        if models_per_task is None:
            models_per_task = self.config.MODELS_PER_TASK

        logger.info(f"ğŸš€ ì „ì²´ íƒœìŠ¤í¬ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"   â€¢ ëŒ€ìƒ íƒœìŠ¤í¬: {tasks}")
        logger.info(f"   â€¢ íƒœìŠ¤í¬ë‹¹ ëª¨ë¸ ìˆ˜: {models_per_task}")

        start_time = time.time()
        self.collection_stats = CollectionStats()

        for task_idx, task in enumerate(tasks, 1):
            try:
                logger.info(f"\nğŸ“‹ [{task_idx}/{len(tasks)}] íƒœìŠ¤í¬ ìˆ˜ì§‘: {task}")

                # ëª¨ë¸ ìˆ˜ì§‘
                models = self.collect_models_by_task(task, models_per_task)

                if not models:
                    logger.warning(f"íƒœìŠ¤í¬ '{task}'ì—ì„œ ìˆ˜ì§‘ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                # ìƒìœ„ ëª¨ë¸ë“¤ì˜ í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘
                top_models = models[:5]  # ìƒìœ„ 5ê°œ ëª¨ë¸ë§Œ
                logger.info(f"ğŸ“Š ìƒìœ„ {len(top_models)}ê°œ ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")

                for model in top_models:
                    self.collect_evaluations_for_model(model.model_id)
                    time.sleep(self.config.API_DELAY)

                logger.info(f"âœ… íƒœìŠ¤í¬ '{task}' ì™„ë£Œ")

            except Exception as e:
                logger.error(f"âŒ íƒœìŠ¤í¬ '{task}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                self.collection_stats.errors_count += 1
                continue

        # ìµœì¢… í†µê³„ ê³„ì‚°
        duration = time.time() - start_time
        self.collection_stats.collection_duration = duration
        self.collection_stats.update_success_rate()

        logger.info(f"\nğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"   â€¢ ì´ ëª¨ë¸: {self.collection_stats.total_models:,}ê°œ")
        logger.info(f"   â€¢ ì´ í‰ê°€ ê²°ê³¼: {self.collection_stats.total_evaluations:,}ê°œ")
        logger.info(f"   â€¢ ìˆ˜ì§‘ íƒœìŠ¤í¬: {', '.join(self.collection_stats.tasks_collected)}")
        logger.info(f"   â€¢ ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
        logger.info(f"   â€¢ ì„±ê³µë¥ : {self.collection_stats.success_rate:.2%}")

        return self.collection_stats

    def generate_task_report(self, task: str, output_dir: Optional[str] = None) -> str:
        """
        íƒœìŠ¤í¬ë³„ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„± (README í˜•ì‹)

        Args:
            task: íƒœìŠ¤í¬ ì´ë¦„
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

        Returns:
            ìƒì„±ëœ ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ
        """
        if output_dir is None:
            output_dir = self.config.REPORTS_DIR

        logger.info(f"ğŸ“„ íƒœìŠ¤í¬ '{task}' ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

        try:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)

            # ë°ì´í„° ìˆ˜ì§‘
            models_df = self.db_manager.get_models_by_task(task)

            if models_df.empty:
                logger.warning(f"íƒœìŠ¤í¬ '{task}'ì— ëŒ€í•œ ëª¨ë¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return ""

            task_info = self.config.get_task_info(task)

            # ë¦¬í¬íŠ¸ ë‚´ìš© ìƒì„±
            report_content = self._generate_report_content(task, models_df, task_info)

            # íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_filename = f"{task}_report_{timestamp}.md"
            report_path = output_path / report_filename

            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)

            logger.info(f"âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
            return str(report_path)

        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    def generate_leaderboard(self, task_type: str, metric_name: str,
                             dataset_name: Optional[str] = None,
                             output_dir: Optional[str] = None) -> str:
        """
        íƒœìŠ¤í¬ë³„ ë¦¬ë”ë³´ë“œ ìƒì„± (README í˜•ì‹)
        """
        if output_dir is None:
            output_dir = self.config.REPORTS_DIR

        logger.info(f"ğŸ† ë¦¬ë”ë³´ë“œ ìƒì„±: {task_type} - {metric_name}")

        try:
            # ë¦¬ë”ë³´ë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            leaderboard_df = self.db_manager.get_task_leaderboard(
                task_type, metric_name, dataset_name
            )

            if leaderboard_df.empty:
                logger.warning(f"ë¦¬ë”ë³´ë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {task_type} - {metric_name}")
                return ""

            # ë¦¬ë”ë³´ë“œ ë¦¬í¬íŠ¸ ìƒì„±
            report_content = self._generate_leaderboard_content(
                task_type, metric_name, dataset_name, leaderboard_df
            )

            # íŒŒì¼ ì €ì¥
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"leaderboard_{task_type}_{metric_name}_{timestamp}.md"
            if dataset_name:
                filename = f"leaderboard_{task_type}_{metric_name}_{dataset_name}_{timestamp}.md"

            report_path = output_path / filename

            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)

            logger.info(f"âœ… ë¦¬ë”ë³´ë“œ ì €ì¥ ì™„ë£Œ: {report_path}")
            return str(report_path)

        except Exception as e:
            logger.error(f"ë¦¬ë”ë³´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    def export_data(self, output_dir: Optional[str] = None):
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸° (README ì˜ˆì‹œ êµ¬í˜„)"""
        if output_dir is None:
            output_dir = self.config.EXPORTS_DIR

        logger.info(f"ğŸ“ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹œì‘: {output_dir}")

        try:
            self.db_manager.export_to_csv(output_dir)
            logger.info(f"âœ… ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_dir}")
        except Exception as e:
            logger.error(f"ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

    def get_collection_summary(self) -> Dict[str, Any]:
        """ìˆ˜ì§‘ í˜„í™© ìš”ì•½ (README ì˜ˆì‹œ êµ¬í˜„)"""
        try:
            stats = self.db_manager.get_model_statistics()

            summary = {
                "database_stats": stats,
                "collection_stats": self.collection_stats.to_dict(),
                "supported_tasks": list(self.config.TASK_CATEGORIES.keys()),
                "last_updated": datetime.now().isoformat()
            }

            return summary
        except Exception as e:
            logger.error(f"ìˆ˜ì§‘ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def _create_model_info(self, model_data: Dict[str, Any], task: Optional[str] = None) -> ModelInfo:
        """API ì‘ë‹µì„ ModelInfo ê°ì²´ë¡œ ë³€í™˜"""
        task_categories = []
        if task:
            task_categories.append(task)

        # íƒœê·¸ì—ì„œ ì¶”ê°€ íƒœìŠ¤í¬ ì¶”ì¶œ
        tags = model_data.get('tags', [])
        for tag in tags:
            if tag in self.config.TASK_CATEGORIES:
                task_categories.append(tag)

        return ModelInfo(
            model_id=model_data.get('id', ''),
            model_name=model_data.get('id', '').split('/')[-1],
            author=model_data.get('author', ''),
            downloads=model_data.get('downloads', 0),
            likes=model_data.get('likes', 0),
            created_at=model_data.get('createdAt', ''),
            last_modified=model_data.get('lastModified', ''),
            library_name=model_data.get('library_name', ''),
            pipeline_tag=model_data.get('pipeline_tag', ''),
            tags=tags,
            task_categories=list(set(task_categories)),  # ì¤‘ë³µ ì œê±°
            model_size=self._extract_model_size(model_data),
            license=model_data.get('cardData', {}).get('license', '')
        )

    def _extract_model_size(self, model_data: Dict[str, Any]) -> Optional[str]:
        """ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶”ì¶œ"""
        tags = model_data.get('tags', [])
        size_patterns = ['7b', '13b', '30b', '65b', '70b', '175b', 'small', 'base', 'large', 'xl']

        for tag in tags:
            tag_lower = tag.lower()
            for pattern in size_patterns:
                if pattern in tag_lower:
                    return tag

        # ëª¨ë¸ ì´ë¦„ì—ì„œ í¬ê¸° ì •ë³´ ì¶”ì¶œ
        model_name = model_data.get('id', '').lower()
        for pattern in size_patterns:
            if pattern in model_name:
                return pattern

        return None

    def _generate_report_content(self, task: str, models_df: pd.DataFrame,
                                 task_info: Dict[str, Any]) -> str:
        """íƒœìŠ¤í¬ ë¦¬í¬íŠ¸ ë‚´ìš© ìƒì„± (README ìŠ¤íƒ€ì¼)"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        content = f"""# {task.upper()} íƒœìŠ¤í¬ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸

**ìƒì„±ì¼:** {timestamp}

## ğŸ“‹ íƒœìŠ¤í¬ ê°œìš”

**ì„¤ëª…:** {task_info.get('description', 'ì„¤ëª… ì—†ìŒ')}

**ì£¼ìš” ë°ì´í„°ì…‹:** {', '.join(task_info.get('common_datasets', []))}

**ì£¼ìš” ë©”íŠ¸ë¦­:** {', '.join(task_info.get('common_metrics', []))}

## ğŸ“Š ìˆ˜ì§‘ í†µê³„

- **ì´ ëª¨ë¸ ìˆ˜:** {len(models_df):,}ê°œ
- **í‰ê·  ë‹¤ìš´ë¡œë“œ ìˆ˜:** {models_df['downloads'].mean():,.0f}
- **í‰ê·  ì¢‹ì•„ìš” ìˆ˜:** {models_df['likes'].mean():,.1f}

## ğŸ† ìƒìœ„ 10ê°œ ëª¨ë¸ (ë‹¤ìš´ë¡œë“œ ìˆœ)

| ìˆœìœ„ | ëª¨ë¸ ID | ì‘ì„±ì | ë‹¤ìš´ë¡œë“œ | ì¢‹ì•„ìš” | í¬ê¸° |
|------|---------|--------|----------|--------|------|
"""

        top_models = models_df.head(10)
        for idx, (_, row) in enumerate(top_models.iterrows(), 1):
            content += f"| {idx} | {row['model_id']} | {row['author']} | {row['downloads']:,} | {row['likes']:,} | {row.get('model_size', 'N/A')} |\n"

        # ì‘ì„±ìë³„ í†µê³„
        author_stats = models_df['author'].value_counts().head(5)
        content += f"\n## ğŸ‘¥ ìƒìœ„ ì‘ì„±ì\n\n"
        for author, count in author_stats.items():
            content += f"- **{author}:** {count}ê°œ ëª¨ë¸\n"

        # ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ ë¶„í¬
        if 'library_name' in models_df.columns:
            library_stats = models_df['library_name'].value_counts().head(5)
            content += f"\n## ğŸ”§ ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬\n\n"
            for library, count in library_stats.items():
                if library:
                    content += f"- **{library}:** {count}ê°œ ëª¨ë¸\n"

        content += f"\n---\n*ì´ ë¦¬í¬íŠ¸ëŠ” HuggingFace LLM í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*"

        return content

    def _generate_leaderboard_content(self, task_type: str, metric_name: str,
                                      dataset_name: Optional[str], leaderboard_df: pd.DataFrame) -> str:
        """ë¦¬ë”ë³´ë“œ ë‚´ìš© ìƒì„± (README ìŠ¤íƒ€ì¼)"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        title = f"{task_type.upper()} - {metric_name.upper()}"
        if dataset_name:
            title += f" ({dataset_name})"

        content = f"""# ğŸ† {title} ë¦¬ë”ë³´ë“œ

**ìƒì„±ì¼:** {timestamp}

**íƒœìŠ¤í¬:** {task_type}
**ë©”íŠ¸ë¦­:** {metric_name}
"""

        if dataset_name:
            content += f"**ë°ì´í„°ì…‹:** {dataset_name}\n"

        content += f"\n**ì´ ëª¨ë¸ ìˆ˜:** {len(leaderboard_df)}ê°œ\n\n"

        # ë¦¬ë”ë³´ë“œ í…Œì´ë¸”
        content += "## ğŸ“ˆ ìˆœìœ„\n\n"
        content += "| ìˆœìœ„ | ëª¨ë¸ | ì‘ì„±ì | ì ìˆ˜ | ë‹¤ìš´ë¡œë“œ | ê²€ì¦ë¨ |\n"
        content += "|------|------|--------|------|----------|--------|\n"

        for idx, (_, row) in enumerate(leaderboard_df.iterrows(), 1):
            verified = "âœ…" if row.get('verified', False) else "âŒ"
            content += f"| {idx} | {row['model_id']} | {row['author']} | {row['metric_value']:.4f} | {row['downloads']:,} | {verified} |\n"

        content += f"\n---\n*ì´ ë¦¬ë”ë³´ë“œëŠ” HuggingFace LLM í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*"

        return content

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ”’ ìˆ˜ì§‘ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        try:
            self.db_manager.close()
            logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# READMEì˜ í¸ì˜ í•¨ìˆ˜ë“¤ êµ¬í˜„
def quick_collect_task(task: str, limit: int = 20, hf_token: Optional[str] = None) -> List[ModelInfo]:
    """
    íŠ¹ì • íƒœìŠ¤í¬ì˜ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ìˆ˜ì§‘ (README ì˜ˆì‹œ êµ¬í˜„)

    Args:
        task: íƒœìŠ¤í¬ ì´ë¦„
        limit: ìˆ˜ì§‘í•  ëª¨ë¸ ìˆ˜
        hf_token: HuggingFace API í† í°

    Returns:
        ìˆ˜ì§‘ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    """
    collector = LLMEvaluationCollector(hf_token=hf_token)
    try:
        logger.info(f"ğŸš€ ë¹ ë¥¸ ìˆ˜ì§‘: {task} ({limit}ê°œ ëª¨ë¸)")
        return collector.collect_models_by_task(task, limit)
    finally:
        collector.close()


def generate_task_summary(task: str) -> Dict[str, Any]:
    """
    íƒœìŠ¤í¬ ìš”ì•½ ì •ë³´ ìƒì„± (README ì˜ˆì‹œ êµ¬í˜„)

    Args:
        task: íƒœìŠ¤í¬ ì´ë¦„

    Returns:
        íƒœìŠ¤í¬ ìš”ì•½ ì •ë³´
    """
    collector = LLMEvaluationCollector()
    try:
        models_df = collector.db_manager.get_models_by_task(task)

        if models_df.empty:
            return {"error": f"íƒœìŠ¤í¬ '{task}'ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        return {
            "task": task,
            "total_models": len(models_df),
            "top_model": models_df.iloc[0]['model_id'] if not models_df.empty else None,
            "avg_downloads": models_df['downloads'].mean(),
            "total_downloads": models_df['downloads'].sum(),
            "unique_authors": models_df['author'].nunique()
        }
    finally:
        collector.close()


def get_model_comparison(model_ids: List[str]) -> pd.DataFrame:
    """
    ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ (README ì˜ˆì‹œ êµ¬í˜„)

    Args:
        model_ids: ë¹„êµí•  ëª¨ë¸ ID ë¦¬ìŠ¤íŠ¸

    Returns:
        ëª¨ë¸ ë¹„êµ ë°ì´í„°í”„ë ˆì„
    """
    collector = LLMEvaluationCollector()
    try:
        comparison_data = []

        for model_id in model_ids:
            evaluations_df = collector.db_manager.get_evaluations_by_model(model_id)

            if not evaluations_df.empty:
                # ê° ë©”íŠ¸ë¦­ì˜ ìµœê³  ì ìˆ˜
                for _, eval_row in evaluations_df.iterrows():
                    comparison_data.append({
                        'model_id': model_id,
                        'dataset': eval_row['dataset_name'],
                        'metric': eval_row['metric_name'],
                        'value': eval_row['metric_value'],
                        'task_type': eval_row['task_type']
                    })

        return pd.DataFrame(comparison_data)
    finally:
        collector.close()