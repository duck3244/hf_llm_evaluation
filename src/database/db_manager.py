"""
ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ëª¨ë“ˆ (ê°œì„ ëœ ë²„ì „)
ë™ì‹œì„± ì²˜ë¦¬ ë° ì•ˆì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê´€ë¦¬
"""

import sqlite3
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
import json
from datetime import datetime
import logging
import threading
import time
from contextlib import contextmanager
import os

from ..models.data_models import (
    ModelInfo, EvaluationResult, TaskCategory, CollectionStats,
    serialize_for_db, deserialize_from_db
)
from ..utils.logger import get_logger, log_database_operation

logger = get_logger(__name__)

# Windows í˜¸í™˜ì„±ì„ ìœ„í•œ ì¡°ê±´ë¶€ import
try:
    import fcntl
    HAS_FCNTL = True
except ImportError:
    HAS_FCNTL = False


class DatabaseError(Exception):
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ì˜ˆì™¸"""
    pass


class DatabaseLockError(DatabaseError):
    """ë°ì´í„°ë² ì´ìŠ¤ ë½ ê´€ë ¨ ì˜ˆì™¸"""
    pass


class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ í´ë˜ìŠ¤ (ë™ì‹œì„± ì²˜ë¦¬ ê°•í™”)"""

    def __init__(self, db_path: str = "data/llm_evaluations.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        # ìŠ¤ë ˆë“œ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ (ì—°ê²° í’€)
        self._local = threading.local()

        # ë½ íŒŒì¼ ê²½ë¡œ
        self.lock_file = self.db_path.with_suffix('.lock')

        # ì—°ê²° ì„¤ì •
        self.connection_timeout = 30.0
        self.max_retries = 3
        self.retry_delay = 0.1

        logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”: {self.db_path}")
        self.init_database()

    def _get_connection(self) -> sqlite3.Connection:
        """ìŠ¤ë ˆë“œ ì•ˆì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° íšë“"""
        if not hasattr(self._local, 'connection') or self._local.connection is None:
            try:
                # WAL ëª¨ë“œë¡œ ì—°ê²° (ë™ì‹œì„± ê°œì„ )
                conn = sqlite3.connect(
                    self.db_path,
                    timeout=self.connection_timeout,
                    check_same_thread=False,
                    isolation_level=None  # autocommit ëª¨ë“œ
                )

                # WAL ëª¨ë“œ ì„¤ì • (Write-Ahead Logging)
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=NORMAL")  # ì„±ëŠ¥ê³¼ ì•ˆì „ì„± ê· í˜•
                conn.execute("PRAGMA cache_size=10000")     # ìºì‹œ í¬ê¸° ì¦ê°€
                conn.execute("PRAGMA temp_store=MEMORY")    # ì„ì‹œ ë°ì´í„° ë©”ëª¨ë¦¬ ì €ì¥
                conn.execute("PRAGMA mmap_size=268435456")  # ë©”ëª¨ë¦¬ ë§µ í¬ê¸° (256MB)

                # ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´ í™œì„±í™”
                conn.execute("PRAGMA foreign_keys=ON")

                # ì—°ê²° ê²€ì¦
                conn.execute("SELECT 1").fetchone()

                self._local.connection = conn
                logger.debug(f"ğŸ”— ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„± (ìŠ¤ë ˆë“œ: {threading.current_thread().ident})")

            except sqlite3.Error as e:
                logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
                raise DatabaseError(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")

        return self._local.connection

    @contextmanager
    def _get_connection_with_retry(self):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        last_exception = None

        for attempt in range(self.max_retries):
            try:
                conn = self._get_connection()

                # ì—°ê²° í…ŒìŠ¤íŠ¸
                conn.execute("BEGIN IMMEDIATE")  # ì¦‰ì‹œ ë°°íƒ€ì  ë½ íšë“
                yield conn
                conn.execute("COMMIT")
                return

            except sqlite3.OperationalError as e:
                if "database is locked" in str(e).lower():
                    last_exception = DatabaseLockError(f"ë°ì´í„°ë² ì´ìŠ¤ ë½ (ì‹œë„ {attempt + 1}): {e}")
                    logger.warning(f"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ë½ ë°œìƒ, ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{self.max_retries})")

                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay * (2 ** attempt))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                else:
                    last_exception = DatabaseError(f"ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ì˜¤ë¥˜: {e}")
                    break

            except Exception as e:
                last_exception = DatabaseError(f"ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}")
                break

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        if last_exception:
            raise last_exception

    @contextmanager
    def _file_lock(self):
        """íŒŒì¼ ê¸°ë°˜ í”„ë¡œì„¸ìŠ¤ ê°„ ë½"""
        if not HAS_FCNTL:  # Windowsì—ì„œëŠ” íŒŒì¼ ë½ ê±´ë„ˆë›°ê¸°
            yield
            return

        lock_fd = None
        try:
            lock_fd = os.open(self.lock_file, os.O_CREAT | os.O_TRUNC | os.O_RDWR)
            fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
            yield
        except (OSError, IOError) as e:
            if lock_fd:
                os.close(lock_fd)
            raise DatabaseLockError(f"í”„ë¡œì„¸ìŠ¤ ë½ íšë“ ì‹¤íŒ¨: {e}")
        finally:
            if lock_fd:
                try:
                    fcntl.flock(lock_fd, fcntl.LOCK_UN)
                    os.close(lock_fd)
                    if self.lock_file.exists():
                        self.lock_file.unlink()
                except:
                    pass

    def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸” ì´ˆê¸°í™” (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._file_lock():
                with self._get_connection_with_retry() as conn:
                    cursor = conn.cursor()

                    # ëª¨ë¸ ì •ë³´ í…Œì´ë¸”
                    cursor.execute("""
                        CREATE TABLE IF NOT EXISTS models (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            model_id TEXT UNIQUE NOT NULL,
                            model_name TEXT,
                            author TEXT,
                            downloads INTEGER DEFAULT 0,
                            likes INTEGER DEFAULT 0,
                            created_at TEXT,
                            last_modified TEXT,
                            library_name TEXT,
                            pipeline_tag TEXT,
                            tags TEXT,
                            task_categories TEXT,
                            model_size TEXT,
                            license TEXT,
                            collected_at TEXT DEFAULT CURRENT_TIMESTAMP,
                            updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                            CONSTRAINT chk_downloads CHECK (downloads >= 0),
                            CONSTRAINT chk_likes CHECK (likes >= 0)
                        )
                    """)

                    # í‰ê°€ ê²°ê³¼ í…Œì´ë¸”
                    cursor.execute("""
                        CREATE TABLE IF NOT EXISTS evaluations (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            model_id TEXT NOT NULL,
                            dataset_name TEXT NOT NULL,
                            metric_name TEXT NOT NULL,
                            metric_value REAL NOT NULL,
                            task_type TEXT,
                            evaluation_date TEXT,
                            dataset_version TEXT,
                            metric_config TEXT,
                            additional_info TEXT,
                            verified BOOLEAN DEFAULT FALSE,
                            collected_at TEXT DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY (model_id) REFERENCES models (model_id) ON DELETE CASCADE,
                            CONSTRAINT chk_metric_value CHECK (metric_value IS NOT NULL AND metric_value = metric_value),
                            UNIQUE(model_id, dataset_name, metric_name, dataset_version)
                        )
                    """)

                    # íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ í…Œì´ë¸”
                    cursor.execute("""
                        CREATE TABLE IF NOT EXISTS task_categories (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            task_name TEXT UNIQUE NOT NULL,
                            description TEXT NOT NULL,
                            common_datasets TEXT,
                            common_metrics TEXT,
                            subcategories TEXT,
                            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                            CONSTRAINT chk_task_name CHECK (LENGTH(task_name) > 0),
                            CONSTRAINT chk_description CHECK (LENGTH(description) > 0)
                        )
                    """)

                    # ë°ì´í„°ì…‹ ì •ë³´ í…Œì´ë¸”
                    cursor.execute("""
                        CREATE TABLE IF NOT EXISTS datasets (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            dataset_name TEXT UNIQUE NOT NULL,
                            task_type TEXT,
                            description TEXT,
                            size INTEGER,
                            language TEXT,
                            splits TEXT,
                            metrics TEXT,
                            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                            CONSTRAINT chk_dataset_name CHECK (LENGTH(dataset_name) > 0),
                            CONSTRAINT chk_size CHECK (size IS NULL OR size >= 0)
                        )
                    """)

                    # ìˆ˜ì§‘ í†µê³„ í…Œì´ë¸”
                    cursor.execute("""
                        CREATE TABLE IF NOT EXISTS collection_stats (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            collection_date TEXT NOT NULL,
                            total_models INTEGER DEFAULT 0,
                            total_evaluations INTEGER DEFAULT 0,
                            tasks_collected TEXT,
                            collection_duration REAL,
                            errors_count INTEGER DEFAULT 0,
                            success_rate REAL DEFAULT 0.0,
                            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                            CONSTRAINT chk_totals CHECK (
                                total_models >= 0 AND 
                                total_evaluations >= 0 AND 
                                errors_count >= 0 AND
                                success_rate >= 0.0 AND 
                                success_rate <= 1.0
                            )
                        )
                    """)

                    # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
                    indexes = [
                        "CREATE INDEX IF NOT EXISTS idx_models_pipeline_tag ON models (pipeline_tag)",
                        "CREATE INDEX IF NOT EXISTS idx_models_downloads ON models (downloads DESC)",
                        "CREATE INDEX IF NOT EXISTS idx_models_author ON models (author)",
                        "CREATE INDEX IF NOT EXISTS idx_models_updated ON models (updated_at DESC)",
                        "CREATE INDEX IF NOT EXISTS idx_evaluations_model_id ON evaluations (model_id)",
                        "CREATE INDEX IF NOT EXISTS idx_evaluations_task_type ON evaluations (task_type)",
                        "CREATE INDEX IF NOT EXISTS idx_evaluations_metric ON evaluations (metric_name)",
                        "CREATE INDEX IF NOT EXISTS idx_evaluations_value ON evaluations (metric_value DESC)",
                        "CREATE INDEX IF NOT EXISTS idx_evaluations_dataset ON evaluations (dataset_name)",
                        "CREATE INDEX IF NOT EXISTS idx_evaluations_composite ON evaluations (task_type, metric_name, metric_value DESC)"
                    ]

                    for index_sql in indexes:
                        cursor.execute(index_sql)

                    # íŠ¸ë¦¬ê±° ìƒì„± (ìë™ ì—…ë°ì´íŠ¸)
                    cursor.execute("""
                        CREATE TRIGGER IF NOT EXISTS update_models_timestamp 
                        AFTER UPDATE ON models
                        BEGIN
                            UPDATE models SET updated_at = CURRENT_TIMESTAMP 
                            WHERE id = NEW.id;
                        END
                    """)

                    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”, ì¸ë±ìŠ¤ ë° íŠ¸ë¦¬ê±° ì´ˆê¸°í™” ì™„ë£Œ")

        except DatabaseLockError as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë½ ì˜¤ë¥˜: {e}")
            raise
        except sqlite3.Error as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise DatabaseError(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def insert_model(self, model_info: ModelInfo) -> bool:
        """ëª¨ë¸ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                cursor = conn.cursor()

                cursor.execute("""
                    INSERT OR REPLACE INTO models 
                    (model_id, model_name, author, downloads, likes, created_at, 
                     last_modified, library_name, pipeline_tag, tags, task_categories,
                     model_size, license, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    model_info.model_id,
                    model_info.model_name,
                    model_info.author,
                    model_info.downloads,
                    model_info.likes,
                    model_info.created_at,
                    model_info.last_modified,
                    model_info.library_name,
                    model_info.pipeline_tag,
                    serialize_for_db(model_info.tags),
                    serialize_for_db(model_info.task_categories),
                    model_info.model_size,
                    model_info.license,
                    datetime.now().isoformat()
                ))

                log_database_operation("INSERT", "models", 1, logger)
                logger.debug(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_info.model_id}")
                return True

        except DatabaseLockError as e:
            logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ë½ ì˜¤ë¥˜ ({model_info.model_id}): {e}")
            return False
        except sqlite3.IntegrityError as e:
            logger.warning(f"âš ï¸  ëª¨ë¸ ì¤‘ë³µ ë˜ëŠ” ì œì•½ ì¡°ê±´ ìœ„ë°˜ ({model_info.model_id}): {e}")
            return False
        except sqlite3.Error as e:
            logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨ ({model_info.model_id}): {e}")
            return False

    def insert_evaluation(self, evaluation: EvaluationResult) -> bool:
        """í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                cursor = conn.cursor()

                cursor.execute("""
                    INSERT OR REPLACE INTO evaluations 
                    (model_id, dataset_name, metric_name, metric_value, 
                     task_type, evaluation_date, dataset_version, metric_config,
                     additional_info, verified)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    evaluation.model_id,
                    evaluation.dataset_name,
                    evaluation.metric_name,
                    evaluation.metric_value,
                    evaluation.task_type,
                    evaluation.evaluation_date,
                    evaluation.dataset_version,
                    serialize_for_db(evaluation.metric_config),
                    serialize_for_db(evaluation.additional_info),
                    evaluation.verified
                ))

                log_database_operation("INSERT", "evaluations", 1, logger)
                logger.debug(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥: {evaluation.model_id} - {evaluation.metric_name}")
                return True

        except DatabaseLockError as e:
            logger.error(f"âŒ í‰ê°€ ê²°ê³¼ ì €ì¥ ë½ ì˜¤ë¥˜: {e}")
            return False
        except sqlite3.IntegrityError as e:
            logger.warning(f"âš ï¸  í‰ê°€ ê²°ê³¼ ì¤‘ë³µ ë˜ëŠ” ì œì•½ ì¡°ê±´ ìœ„ë°˜: {e}")
            return False
        except sqlite3.Error as e:
            logger.error(f"âŒ í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def insert_task_category(self, task_category: TaskCategory) -> bool:
        """íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì €ì¥ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                cursor = conn.cursor()

                cursor.execute("""
                    INSERT OR REPLACE INTO task_categories 
                    (task_name, description, common_datasets, common_metrics, subcategories)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    task_category.task_name,
                    task_category.description,
                    serialize_for_db(task_category.common_datasets),
                    serialize_for_db(task_category.common_metrics),
                    serialize_for_db(task_category.subcategories)
                ))

                log_database_operation("INSERT", "task_categories", 1, logger)
                return True

        except DatabaseLockError as e:
            logger.error(f"âŒ íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ ì €ì¥ ë½ ì˜¤ë¥˜: {e}")
            return False
        except sqlite3.Error as e:
            logger.error(f"âŒ íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def get_models_by_task(self, task: str, limit: Optional[int] = None) -> pd.DataFrame:
        """íŠ¹ì • íƒœìŠ¤í¬ì˜ ëª¨ë¸ë“¤ì„ ì¡°íšŒ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                query = """
                    SELECT * FROM models 
                    WHERE pipeline_tag = ? OR task_categories LIKE ?
                    ORDER BY downloads DESC
                """
                params = [task, f'%{task}%']

                if limit:
                    query += " LIMIT ?"
                    params.append(limit)

                df = pd.read_sql_query(query, conn, params=params)

                # JSON ì»¬ëŸ¼ ì—­ì§ë ¬í™”
                if not df.empty:
                    df['tags'] = df['tags'].apply(lambda x: deserialize_from_db(x, list) if x else [])
                    df['task_categories'] = df['task_categories'].apply(
                        lambda x: deserialize_from_db(x, list) if x else [])

                logger.debug(f"ğŸ“Š íƒœìŠ¤í¬ '{task}' ëª¨ë¸ ì¡°íšŒ: {len(df)}ê°œ")
                return df

        except DatabaseLockError as e:
            logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ë½ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
        except sqlite3.Error as e:
            logger.error(f"âŒ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def get_evaluations_by_model(self, model_id: str) -> pd.DataFrame:
        """íŠ¹ì • ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                query = "SELECT * FROM evaluations WHERE model_id = ? ORDER BY collected_at DESC"
                df = pd.read_sql_query(query, conn, params=[model_id])

                # JSON ì»¬ëŸ¼ ì—­ì§ë ¬í™”
                if not df.empty:
                    df['additional_info'] = df['additional_info'].apply(
                        lambda x: deserialize_from_db(x, dict) if x else {}
                    )
                    df['metric_config'] = df['metric_config'].apply(
                        lambda x: deserialize_from_db(x, dict) if x else {}
                    )

                logger.debug(f"ğŸ“Š ëª¨ë¸ '{model_id}' í‰ê°€ ê²°ê³¼ ì¡°íšŒ: {len(df)}ê°œ")
                return df

        except DatabaseLockError as e:
            logger.error(f"âŒ í‰ê°€ ê²°ê³¼ ì¡°íšŒ ë½ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
        except sqlite3.Error as e:
            logger.error(f"âŒ í‰ê°€ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def get_task_leaderboard(self, task_type: str, metric_name: str,
                             dataset_name: Optional[str] = None, limit: int = 50) -> pd.DataFrame:
        """íƒœìŠ¤í¬ë³„ ë¦¬ë”ë³´ë“œë¥¼ ìƒì„± (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                query = """
                    SELECT m.model_id, m.model_name, m.author, e.metric_value, 
                           e.dataset_name, e.evaluation_date, m.downloads, m.likes,
                           m.model_size, m.license, e.verified
                    FROM models m
                    JOIN evaluations e ON m.model_id = e.model_id
                    WHERE e.task_type = ? AND e.metric_name = ?
                """
                params = [task_type, metric_name]

                if dataset_name:
                    query += " AND e.dataset_name = ?"
                    params.append(dataset_name)

                query += " ORDER BY e.metric_value DESC LIMIT ?"
                params.append(limit)

                df = pd.read_sql_query(query, conn, params=params)
                logger.debug(f"ğŸ† ë¦¬ë”ë³´ë“œ ìƒì„±: {task_type}-{metric_name} ({len(df)}ê°œ)")
                return df

        except DatabaseLockError as e:
            logger.error(f"âŒ ë¦¬ë”ë³´ë“œ ì¡°íšŒ ë½ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
        except sqlite3.Error as e:
            logger.error(f"âŒ ë¦¬ë”ë³´ë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def get_model_statistics(self) -> Dict[str, Any]:
        """ëª¨ë¸ í†µê³„ ì •ë³´ ë°˜í™˜ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                cursor = conn.cursor()

                # ì „ì²´ ëª¨ë¸ ìˆ˜
                cursor.execute("SELECT COUNT(*) FROM models")
                total_models = cursor.fetchone()[0]

                # íƒœìŠ¤í¬ë³„ ëª¨ë¸ ìˆ˜
                cursor.execute("""
                    SELECT pipeline_tag, COUNT(*) 
                    FROM models 
                    WHERE pipeline_tag IS NOT NULL AND pipeline_tag != ''
                    GROUP BY pipeline_tag 
                    ORDER BY COUNT(*) DESC
                """)
                task_counts = dict(cursor.fetchall())

                # í‰ê°€ ê²°ê³¼ ìˆ˜
                cursor.execute("SELECT COUNT(*) FROM evaluations")
                total_evaluations = cursor.fetchone()[0]

                # ìµœê·¼ ìˆ˜ì§‘ ë‚ ì§œ
                cursor.execute("SELECT MAX(collected_at) FROM models")
                last_collection = cursor.fetchone()[0]

                # ìƒìœ„ ì‘ì„±ì
                cursor.execute("""
                    SELECT author, COUNT(*) as model_count
                    FROM models 
                    WHERE author IS NOT NULL AND author != ''
                    GROUP BY author 
                    ORDER BY model_count DESC 
                    LIMIT 5
                """)
                top_authors = dict(cursor.fetchall())

                stats = {
                    'total_models': total_models,
                    'total_evaluations': total_evaluations,
                    'task_counts': task_counts,
                    'last_collection': last_collection,
                    'top_authors': top_authors
                }

                logger.debug(f"ğŸ“Š í†µê³„ ì¡°íšŒ ì™„ë£Œ: {total_models:,}ê°œ ëª¨ë¸, {total_evaluations:,}ê°œ í‰ê°€")
                return stats

        except DatabaseLockError as e:
            logger.error(f"âŒ í†µê³„ ì¡°íšŒ ë½ ì˜¤ë¥˜: {e}")
            return {}
        except sqlite3.Error as e:
            logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def search_models(self, query: str, limit: int = 50) -> pd.DataFrame:
        """ëª¨ë¸ ê²€ìƒ‰ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                sql_query = """
                    SELECT * FROM models 
                    WHERE model_id LIKE ? OR model_name LIKE ? OR author LIKE ?
                    ORDER BY downloads DESC
                    LIMIT ?
                """
                search_term = f"%{query}%"
                params = [search_term, search_term, search_term, limit]

                df = pd.read_sql_query(sql_query, conn, params=params)
                logger.debug(f"ğŸ” ëª¨ë¸ ê²€ìƒ‰ '{query}': {len(df)}ê°œ ê²°ê³¼")
                return df

        except DatabaseLockError as e:
            logger.error(f"âŒ ëª¨ë¸ ê²€ìƒ‰ ë½ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
        except sqlite3.Error as e:
            logger.error(f"âŒ ëª¨ë¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def get_top_models_by_downloads(self, limit: int = 50, task: Optional[str] = None) -> pd.DataFrame:
        """ë‹¤ìš´ë¡œë“œ ìˆ˜ ê¸°ì¤€ ìƒìœ„ ëª¨ë¸ (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._get_connection_with_retry() as conn:
                query = "SELECT * FROM models"
                params = []

                if task:
                    query += " WHERE pipeline_tag = ?"
                    params.append(task)

                query += " ORDER BY downloads DESC LIMIT ?"
                params.append(limit)

                df = pd.read_sql_query(query, conn, params=params)
                logger.debug(f"ğŸ† ìƒìœ„ ëª¨ë¸ ì¡°íšŒ: {len(df)}ê°œ")
                return df

        except DatabaseLockError as e:
            logger.error(f"âŒ ìƒìœ„ ëª¨ë¸ ì¡°íšŒ ë½ ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
        except sqlite3.Error as e:
            logger.error(f"âŒ ìƒìœ„ ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def export_to_csv(self, output_dir: str = "exports"):
        """ë°ì´í„°ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸° (ë™ì‹œì„± ì•ˆì „)"""
        try:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
            exported_files = []

            with self._get_connection_with_retry() as conn:
                # ëª¨ë¸ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
                models_df = pd.read_sql_query("SELECT * FROM models ORDER BY downloads DESC", conn)
                if not models_df.empty:
                    models_file = output_path / "models.csv"
                    models_df.to_csv(models_file, index=False, encoding='utf-8')
                    exported_files.append(str(models_file))

                # í‰ê°€ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
                evaluations_df = pd.read_sql_query("SELECT * FROM evaluations ORDER BY collected_at DESC", conn)
                if not evaluations_df.empty:
                    evaluations_file = output_path / "evaluations.csv"
                    evaluations_df.to_csv(evaluations_file, index=False, encoding='utf-8')
                    exported_files.append(str(evaluations_file))

                # íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬ ë‚´ë³´ë‚´ê¸°
                tasks_df = pd.read_sql_query("SELECT * FROM task_categories", conn)
                if not tasks_df.empty:
                    tasks_file = output_path / "task_categories.csv"
                    tasks_df.to_csv(tasks_file, index=False, encoding='utf-8')
                    exported_files.append(str(tasks_file))

                # ìˆ˜ì§‘ í†µê³„ ë‚´ë³´ë‚´ê¸°
                stats_df = pd.read_sql_query("SELECT * FROM collection_stats ORDER BY created_at DESC", conn)
                if not stats_df.empty:
                    stats_file = output_path / "collection_stats.csv"
                    stats_df.to_csv(stats_file, index=False, encoding='utf-8')
                    exported_files.append(str(stats_file))

            logger.info(f"ğŸ“ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {len(exported_files)}ê°œ íŒŒì¼")
            for file_path in exported_files:
                logger.info(f"   â€¢ {file_path}")

        except DatabaseLockError as e:
            logger.error(f"âŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ë½ ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

    def backup_database(self, backup_path: Optional[str] = None):
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (ë™ì‹œì„± ì•ˆì „)"""
        if not backup_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"data/backup_llm_evaluations_{timestamp}.db"

        try:
            backup_path_obj = Path(backup_path)
            backup_path_obj.parent.mkdir(parents=True, exist_ok=True)

            with self._get_connection_with_retry() as conn:
                # SQLite ë°±ì—… API ì‚¬ìš© (ì›ìì  ë°±ì—…)
                backup_conn = sqlite3.connect(backup_path)
                try:
                    conn.backup(backup_conn)
                    logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: {backup_path}")
                    return backup_path
                finally:
                    backup_conn.close()

        except DatabaseLockError as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ë½ ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨: {e}")
            return None

    def vacuum_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” (ë™ì‹œì„± ì•ˆì „)"""
        try:
            with self._file_lock():  # ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ë½ í•„ìš”
                with self._get_connection_with_retry() as conn:
                    logger.info("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹œì‘...")

                    # WAL ì²´í¬í¬ì¸íŠ¸
                    conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")

                    # VACUUM ì‹¤í–‰
                    conn.execute("VACUUM")

                    # í†µê³„ ì—…ë°ì´íŠ¸
                    conn.execute("ANALYZE")

                    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì™„ë£Œ")

        except DatabaseLockError as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ë½ ì˜¤ë¥˜: {e}")
        except sqlite3.Error as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")

    def get_database_size(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì •ë³´ ì¡°íšŒ"""
        try:
            file_size = self.db_path.stat().st_size
            size_mb = file_size / (1024 * 1024)

            with self._get_connection_with_retry() as conn:
                cursor = conn.cursor()

                # í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜
                tables = ['models', 'evaluations', 'task_categories', 'collection_stats']
                table_counts = {}

                for table in tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    table_counts[table] = cursor.fetchone()[0]

                # WAL íŒŒì¼ í¬ê¸°
                wal_file = self.db_path.with_suffix('.db-wal')
                wal_size = 0
                if wal_file.exists():
                    wal_size = wal_file.stat().st_size

            return {
                'file_size_bytes': file_size,
                'file_size_mb': round(size_mb, 2),
                'wal_size_bytes': wal_size,
                'wal_size_mb': round(wal_size / (1024 * 1024), 2),
                'table_counts': table_counts,
                'db_path': str(self.db_path)
            }

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def health_check(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì ê²€"""
        try:
            start_time = time.time()

            with self._get_connection_with_retry() as conn:
                cursor = conn.cursor()

                # ì—°ê²° í…ŒìŠ¤íŠ¸
                cursor.execute("SELECT 1")

                # ë¬´ê²°ì„± ê²€ì‚¬ (ë¹ ë¥¸ ê²€ì‚¬)
                cursor.execute("PRAGMA quick_check")
                integrity_result = cursor.fetchone()[0]

                # WAL ëª¨ë“œ í™•ì¸
                cursor.execute("PRAGMA journal_mode")
                journal_mode = cursor.fetchone()[0]

                # ì™¸ë˜ í‚¤ ì œì•½ ì¡°ê±´ í™•ì¸
                cursor.execute("PRAGMA foreign_keys")
                foreign_keys = cursor.fetchone()[0]

            response_time = time.time() - start_time

            health = {
                'status': 'healthy' if integrity_result == 'ok' else 'unhealthy',
                'response_time': round(response_time, 3),
                'integrity_check': integrity_result,
                'journal_mode': journal_mode,
                'foreign_keys_enabled': bool(foreign_keys),
                'connection_timeout': self.connection_timeout,
                'wal_enabled': journal_mode.upper() == 'WAL'
            }

            if health['status'] == 'healthy':
                logger.debug("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì •ìƒ")
            else:
                logger.warning(f"âš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì´ìƒ: {integrity_result}")

            return health

        except DatabaseLockError as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì ê²€ ë½ ì˜¤ë¥˜: {e}")
            return {'status': 'error', 'error': str(e)}
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ì ê²€ ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'error': str(e)}

    def get_connection_info(self) -> Dict[str, Any]:
        """í˜„ì¬ ì—°ê²° ì •ë³´ ë°˜í™˜"""
        try:
            thread_id = threading.current_thread().ident
            has_connection = hasattr(self._local, 'connection') and self._local.connection is not None

            info = {
                'thread_id': thread_id,
                'has_local_connection': has_connection,
                'db_path': str(self.db_path),
                'connection_timeout': self.connection_timeout,
                'max_retries': self.max_retries,
                'lock_file_exists': self.lock_file.exists()
            }

            if has_connection:
                try:
                    # ì—°ê²° ìƒíƒœ í™•ì¸
                    self._local.connection.execute("SELECT 1")
                    info['connection_status'] = 'active'
                except:
                    info['connection_status'] = 'inactive'
            else:
                info['connection_status'] = 'none'

            return info

        except Exception as e:
            logger.error(f"âŒ ì—°ê²° ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}

    def close_connection(self):
        """í˜„ì¬ ìŠ¤ë ˆë“œì˜ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if hasattr(self._local, 'connection') and self._local.connection is not None:
            try:
                self._local.connection.close()
                self._local.connection = None
                logger.debug(f"ğŸ”’ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ (ìŠ¤ë ˆë“œ: {threading.current_thread().ident})")
            except Exception as e:
                logger.error(f"âŒ ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì ì¢…ë£Œ"""
        try:
            self.close_connection()

            # ë½ íŒŒì¼ ì •ë¦¬
            if self.lock_file.exists():
                try:
                    self.lock_file.unlink()
                except:
                    pass

            logger.debug("ğŸ”’ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì ì¢…ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.close()

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            self.close()
        except:
            pass