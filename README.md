# HuggingFace LLM ì„±ëŠ¥ í‰ê°€ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì íŠ¸

HuggingFace Hubì—ì„œ LLM ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ í‰ê°€ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ëŠ” ì¢…í•©ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤.

## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥

- **ìë™ ë°ì´í„° ìˆ˜ì§‘**: HuggingFace APIë¥¼ í†µí•œ ëª¨ë¸ ì •ë³´ ë° í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘
- **íƒœìŠ¤í¬ë³„ ë¶„ë¥˜**: í…ìŠ¤íŠ¸ ìƒì„±, ë¶„ë¥˜, ì§ˆë¬¸ ë‹µë³€ ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë³„ ëª¨ë¸ ê´€ë¦¬
- **ì„±ëŠ¥ ë¶„ì„**: íƒœìŠ¤í¬ë³„ ë¦¬ë”ë³´ë“œ ë° ìƒì„¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
- **ë°ì´í„° ì €ì¥**: SQLiteë¥¼ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ë°ì´í„° ê´€ë¦¬
- **ë‚´ë³´ë‚´ê¸°**: CSV í˜•íƒœë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì§€ì›

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/yourusername/hf-llm-evaluation.git
cd hf-llm-evaluation

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
cp .env.example .env
# .env íŒŒì¼ì—ì„œ HUGGINGFACE_TOKEN ì„¤ì •
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# í˜„ì¬ ìˆ˜ì§‘ í˜„í™© í™•ì¸
python main.py --stats

# ëª¨ë“  íƒœìŠ¤í¬ ë°ì´í„° ìˆ˜ì§‘
python main.py --collect-all

# íŠ¹ì • íƒœìŠ¤í¬ë§Œ ìˆ˜ì§‘
python main.py --task text-generation

# ë¦¬í¬íŠ¸ ìƒì„±
python main.py --reports

# ë°ì´í„° ë‚´ë³´ë‚´ê¸°
python main.py --export
```

### 3. í”„ë¡œê·¸ë˜ë° ë°©ì‹ ì‚¬ìš©

```python
from src.collectors.evaluation_collector import LLMEvaluationCollector

# ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
collector = LLMEvaluationCollector()

# íŠ¹ì • íƒœìŠ¤í¬ ëª¨ë¸ ìˆ˜ì§‘
models = collector.collect_models_by_task("text-generation", limit=10)

# ë¦¬ë”ë³´ë“œ ìƒì„±
leaderboard = collector.db_manager.get_task_leaderboard(
    task_type="text-generation", 
    metric_name="accuracy"
)

print(leaderboard.head())
```

## ğŸ“Š ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬

| íƒœìŠ¤í¬ | ì„¤ëª… | ì£¼ìš” ë°ì´í„°ì…‹ | ì£¼ìš” ë©”íŠ¸ë¦­ |
|--------|------|---------------|-------------|
| text-generation | í…ìŠ¤íŠ¸ ìƒì„± | hellaswag, arc, mmlu | accuracy, perplexity |
| text-classification | í…ìŠ¤íŠ¸ ë¶„ë¥˜ | glue, imdb, sst2 | accuracy, f1 |
| question-answering | ì§ˆë¬¸ ë‹µë³€ | squad, natural_questions | exact_match, f1 |
| summarization | ìš”ì•½ | cnn_dailymail, xsum | rouge-1, rouge-2 |
| translation | ë²ˆì—­ | wmt14, opus | bleu, meteor |

## ğŸ—‚ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
hf_llm_evaluation/
â”œâ”€â”€ README.md                    # í”„ë¡œì íŠ¸ ë¬¸ì„œ
â”œâ”€â”€ requirements.txt             # Python ì˜ì¡´ì„±
â”œâ”€â”€ config.py                    # ì„¤ì • íŒŒì¼
â”œâ”€â”€ main.py                      # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ src/                         # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ models/                  # ë°ì´í„° ëª¨ë¸
â”‚   â”‚   â””â”€â”€ data_models.py
â”‚   â”œâ”€â”€ api/                     # API í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â””â”€â”€ huggingface_api.py
â”‚   â”œâ”€â”€ database/                # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ db_manager.py
â”‚   â”œâ”€â”€ collectors/              # ë°ì´í„° ìˆ˜ì§‘ê¸°
â”‚   â”‚   â””â”€â”€ evaluation_collector.py
â”‚   â””â”€â”€ utils/                   # ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ data/                        # ë°ì´í„° ì €ì¥ì†Œ
â”‚   â””â”€â”€ llm_evaluations.db
â”œâ”€â”€ reports/                     # ìƒì„±ëœ ë¦¬í¬íŠ¸
â””â”€â”€ exports/                     # ë‚´ë³´ë‚¸ ë°ì´í„°
```

## âš™ï¸ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜ (.env)

```bash
# HuggingFace API í† í° (ì„ íƒì‚¬í•­, ë” ë§ì€ API í˜¸ì¶œ í—ˆìš©)
HUGGINGFACE_TOKEN=your_token_here

# ë¡œê·¸ ë ˆë²¨
LOG_LEVEL=INFO

# API ìš”ì²­ ê°„ê²© (ì´ˆ)
API_DELAY=0.1

# íƒœìŠ¤í¬ë³„ ìˆ˜ì§‘í•  ëª¨ë¸ ìˆ˜
MODELS_PER_TASK=30
```

### ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§• (config.py)

```python
# ìˆ˜ì§‘í•  íƒœìŠ¤í¬ ìˆ˜ì •
TASKS_TO_COLLECT = [
    "text-generation",
    "text-classification", 
    "question-answering"
]

# íƒœìŠ¤í¬ë³„ ëª¨ë¸ ìˆ˜ ì¡°ì •
MODELS_PER_TASK = 50

# API ìš”ì²­ ê°„ê²© ì¡°ì •
API_DELAY = 0.2
```

## ğŸ“ˆ ì‚¬ìš© ì˜ˆì‹œ

### 1. ëŒ€í™”í˜• ë¶„ì„

```python
from src.collectors.evaluation_collector import LLMEvaluationCollector
import pandas as pd

collector = LLMEvaluationCollector()

# í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ ìƒìœ„ 10ê°œ ì¡°íšŒ
models = collector.db_manager.get_top_models_by_downloads(
    limit=10, 
    task="text-generation"
)

print("ìƒìœ„ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸:")
print(models[['model_id', 'downloads', 'likes']].head())

# íŠ¹ì • ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ ì¡°íšŒ
evaluations = collector.db_manager.get_evaluations_by_model(
    "microsoft/DialoGPT-medium"
)

print("\ní‰ê°€ ê²°ê³¼:")
print(evaluations[['dataset_name', 'metric_name', 'metric_value']])
```

### 2. ë°°ì¹˜ ìˆ˜ì§‘

```python
# íŠ¹ì • íƒœìŠ¤í¬ë“¤ë§Œ ìˆ˜ì§‘
tasks_to_collect = ["text-generation", "question-answering"]

for task in tasks_to_collect:
    collector.collect_models_by_task(task, limit=20)
    
# ë¦¬í¬íŠ¸ ì¼ê´„ ìƒì„±
for task in tasks_to_collect:
    collector.generate_task_report(task)
```

### 3. ì»¤ìŠ¤í…€ ë¶„ì„

```python
# ë‹¤ìš´ë¡œë“œ ìˆ˜ vs ì„±ëŠ¥ ë¶„ì„
models_df = collector.db_manager.get_models_by_task("text-generation")
evaluations_df = collector.db_manager.get_evaluations_by_model("gpt2")

# ë‘ ë°ì´í„°í”„ë ˆì„ ë³‘í•©í•˜ì—¬ ë¶„ì„
merged_df = models_df.merge(
    evaluations_df, 
    left_on='model_id', 
    right_on='model_id'
)

# ë‹¤ìš´ë¡œë“œ ìˆ˜ì™€ ì„±ëŠ¥ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
correlation = merged_df['downloads'].corr(merged_df['metric_value'])
print(f"ë‹¤ìš´ë¡œë“œ ìˆ˜ì™€ ì„±ëŠ¥ì˜ ìƒê´€ê´€ê³„: {correlation:.3f}")
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### 1. ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì¿¼ë¦¬

```python
import sqlite3
import pandas as pd

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
conn = sqlite3.connect("data/llm_evaluations.db")

# ì»¤ìŠ¤í…€ ì¿¼ë¦¬
query = """
SELECT m.model_id, m.downloads, e.metric_value
FROM models m
JOIN evaluations e ON m.model_id = e.model_id
WHERE e.metric_name = 'accuracy' AND e.metric_value > 0.8
ORDER BY m.downloads DESC
"""

results = pd.read_sql_query(query, conn)
print(results)
```

### 2. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```python
import time
from src.utils.logger import ProgressLogger, get_logger

logger = get_logger(__name__)

def monitor_collection():
    collector = LLMEvaluationCollector()
    
    while True:
        stats = collector.get_collection_summary()
        logger.info(f"í˜„ì¬ ëª¨ë¸ ìˆ˜: {stats['database_stats']['total_models']}")
        
        # 1ì‹œê°„ë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸ í™•ì¸
        time.sleep(3600)
```

### 3. ë°ì´í„° ê²€ì¦

```python
from src.models.data_models import validate_model_info, validate_evaluation_result

# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
def validate_database():
    collector = LLMEvaluationCollector()
    
    # ëª¨ë“  ëª¨ë¸ ì •ë³´ ê²€ì¦
    models_df = collector.db_manager.get_top_models_by_downloads(limit=1000)
    
    invalid_models = []
    for _, row in models_df.iterrows():
        model_info = ModelInfo(**row.to_dict())
        if not validate_model_info(model_info):
            invalid_models.append(model_info.model_id)
    
    print(f"ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸: {len(invalid_models)}ê°œ")
    return invalid_models
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

1. **API Rate Limit ì˜¤ë¥˜**
   ```bash
   # API ìš”ì²­ ê°„ê²©ì„ ëŠ˜ë ¤ì£¼ì„¸ìš”
   export API_DELAY=0.5
   ```

2. **ë°ì´í„°ë² ì´ìŠ¤ ë½ ì˜¤ë¥˜**
   ```python
   # ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í›„ ì¬ìƒì„±
   collector.db_manager.backup_database()
   ```

3. **ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```bash
   # ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”
   python main.py --task text-generation --limit 10
   ```

### ë¡œê·¸ í™•ì¸

```bash
# ìƒì„¸ ë¡œê¹… í™œì„±í™”
python main.py --collect-all --verbose

# ë¡œê·¸ íŒŒì¼ í™•ì¸
tail -f logs/hf_llm_evaluation_*.log
```

**Happy Evaluating! ğŸš€**